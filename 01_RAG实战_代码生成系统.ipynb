{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据抓取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "from pathlib import Path\n",
    "\n",
    "def clone_repos(repo_list, target_dir):\n",
    "    for repo_url in repo_list:\n",
    "        repo_name = repo_url.split('/')[-1].replace('.git', '')\n",
    "        repo_path = Path(target_dir) / repo_name\n",
    "        if not repo_path.exists():\n",
    "            git.Repo.clone_from(repo_url, repo_path)\n",
    "        else:\n",
    "            repo = git.Repo(repo_path)\n",
    "            repo.remotes.origin.pull()\n",
    "\n",
    "# 使用示例\n",
    "repo_list = ['https://github.com/LGRY/self-llm.git']\n",
    "clone_repos(repo_list, './raw_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import astroid\n",
    "from typing import List\n",
    "\n",
    "def clean_python_code(code: str) -> str:\n",
    "    # 移除注释\n",
    "    tree = ast.parse(code)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):\n",
    "            node.value.s = \"\"\n",
    "\n",
    "    # 移除空行\n",
    "    cleaned_code = ast.unparse(tree)\n",
    "    cleaned_code = \"\\n\".join([line for line in cleaned_code.split(\"\\n\") if line.strip()])\n",
    "\n",
    "    return cleaned_code\n",
    "\n",
    "def remove_sensitive_info(code: str, sensitive_patterns: List[str]) -> str:\n",
    "    # 从给定的代码字符串中移除敏感信息\n",
    "    # 遍历敏感信息模式列表\n",
    "    for pattern in sensitive_patterns:\n",
    "        # 使用空字符串替换代码中的敏感信息模式\n",
    "        code = code.replace(pattern, \"[REDACTED]\")\n",
    "    # 返回处理后的代码字符串\n",
    "    return code\n",
    "\n",
    "# 使用示例\n",
    "raw_code = \"\"\"\n",
    "# This is a comment\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")  # Another comment\n",
    "\n",
    "API_KEY = \"very_secret_key\"\n",
    "\"\"\"\n",
    "\n",
    "sensitive_patterns = [\"very_secret_key\"]\n",
    "cleaned_code = clean_python_code(raw_code)\n",
    "safe_code = remove_sensitive_info(cleaned_code, sensitive_patterns)\n",
    "print(safe_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 代码格式化\n",
    "使用工具如black（Python）或prettier（JavaScript）来标准化代码格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import black\n",
    "\n",
    "def format_python_code(code: str) -> str:\n",
    "    return black.format_str(code, mode=black.FileMode())\n",
    "\n",
    "# 使用示例\n",
    "formatted_code = format_python_code(safe_code)\n",
    "print(formatted_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 命名规范化\n",
    "使用正则表达式统一命名风格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def standardize_naming(code: str, style: str = 'snake_case') -> str:\n",
    "    \"\"\"\n",
    "    将给定的代码字符串标准化为指定的命名风格。\n",
    "\n",
    "    参数:\n",
    "        code (str): 需要标准化的代码字符串。\n",
    "        style (str, 可选): 目标命名风格，默认为'snake_case'。\n",
    "                           可选值: 'snake_case', 'camelCase'。\n",
    "\n",
    "    返回:\n",
    "        str: 标准化后的代码字符串。\n",
    "    \"\"\"\n",
    "    if style == 'snake_case':\n",
    "        # 定义匹配模式，用于查找小写字母或数字后跟大写字母的情况\n",
    "        pattern = r'([a-z0-9])([A-Z])'\n",
    "        # 定义替换模式，在匹配的两个字符之间添加下划线\n",
    "        replacement = r'\\1_\\2'\n",
    "    elif style == 'camelCase':\n",
    "        def camel_case(match):\n",
    "            \"\"\"\n",
    "            将匹配到的下划线和其后的字符转换为驼峰命名法。\n",
    "\n",
    "            参数:\n",
    "                match (re.Match): 正则表达式匹配对象。\n",
    "\n",
    "            返回:\n",
    "                str: 转换后的字符串。\n",
    "            \"\"\"\n",
    "            # 返回第一个匹配组（下划线之前的字符）和第二个匹配组（下划线之后的字符）的大写形式\n",
    "            return match.group(1) + match.group(2).upper()\n",
    "        # 定义匹配模式，用于查找下划线后跟字母的情况\n",
    "        pattern = r'(_)([a-zA-Z])'\n",
    "        # 使用自定义的camel_case函数作为替换模式\n",
    "        replacement = camel_case\n",
    "\n",
    "    # 使用正则表达式替换函数将代码字符串中的匹配模式替换为指定的替换模式\n",
    "    return re.sub(pattern, replacement, code)\n",
    "\n",
    "# 使用示例\n",
    "# 假设formatted_code是一个已经格式化的代码字符串\n",
    "standardized_code = standardize_naming(formatted_code, 'snake_case')\n",
    "print(standardized_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 知识图谱构建\n",
    "### 4.1 实体提取\n",
    "使用AST（抽象语法树）分析代码结构，提取关键实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_entities(code: str):\n",
    "    tree = ast.parse(code)\n",
    "    entities = {\n",
    "        'functions': [],\n",
    "        'classes': [],\n",
    "        'imports': []\n",
    "    }\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            entities['functions'].append(node.name)\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            entities['classes'].append(node.name)\n",
    "        elif isinstance(node, ast.Import):\n",
    "            entities['imports'].extend(alias.name for alias in node.names)\n",
    "\n",
    "    return entities\n",
    "\n",
    "# 使用示例\n",
    "entities = extract_entities(standardized_code)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 关系建模\n",
    "使用NetworkX库构建和可视化知识图谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_knowledge_graph(entities):\n",
    "    \"\"\"\n",
    "    根据提取的实体构建知识图谱。\n",
    "\n",
    "    参数:\n",
    "        entities (dict): 包含提取的实体的字典，格式为:\n",
    "              {\n",
    "                  'functions': [函数名列表],\n",
    "                  'classes': [类名列表],\n",
    "                  'imports': [导入的模块名列表]\n",
    "              }\n",
    "\n",
    "    返回:\n",
    "        networkx.Graph: 构建的知识图谱。\n",
    "    \"\"\"\n",
    "    # 初始化一个无向图\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 遍历实体字典中的每个实体类型和对应的实体列表\n",
    "    for entity_type, items in entities.items():\n",
    "        # 遍历实体列表中的每个实体\n",
    "        for item in items:\n",
    "            # 将实体添加为图的节点，并标记其类型\n",
    "            G.add_node(item, type=entity_type)\n",
    "\n",
    "    # 添加关系（这里简化处理，实际应根据代码分析确定关系）\n",
    "    for func in entities['functions']:\n",
    "        for cls in entities['classes']:\n",
    "            # 在函数和类之间添加一条边，表示函数属于类\n",
    "            G.add_edge(func, cls, relation=\"belongs_to\")\n",
    "\n",
    "    # 返回构建的知识图谱\n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    \"\"\"\n",
    "    使用matplotlib可视化知识图谱。\n",
    "\n",
    "    参数:\n",
    "        G (networkx.Graph): 需要可视化的知识图谱。\n",
    "    \"\"\"\n",
    "    # 使用Spring布局算法计算节点的位置\n",
    "    pos = nx.spring_layout(G)\n",
    "    # 创建一个新的图形，设置图形的大小\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # 绘制知识图谱，设置节点的标签、颜色、大小、字体大小和字体粗细\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=8, font_weight='bold')\n",
    "    # 获取图中边的关系标签\n",
    "    edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "    # 绘制边的关系标签\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    # 设置图形的标题\n",
    "    plt.title(\"Code Knowledge Graph\")\n",
    "    # 关闭坐标轴\n",
    "    plt.axis('off')\n",
    "    # 自动调整子图参数，使之填充整个图像区域\n",
    "    plt.tight_layout()\n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "\n",
    "# 使用示例\n",
    "G = build_knowledge_graph(entities)\n",
    "visualize_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG系统实现\n",
    "### 5.1 文本嵌入\n",
    "使用Sentence Transformers生成文本嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "# 使用示例\n",
    "code_snippets = [standardized_code]  # 实际应用中这里会是多段代码\n",
    "embeddings = generate_embeddings(code_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 向量索引\n",
    "使用FAISS构建向量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# 使用示例\n",
    "index = build_faiss_index(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 检索实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_codes(query, index, embeddings, k=5):\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return [(distances[0][i], embeddings[indices[0][i]]) for i in range(k)]\n",
    "\n",
    "# 使用示例\n",
    "query = \"How to implement a binary search tree?\"\n",
    "similar_codes = retrieve_similar_codes(query, index, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 代码生成模型训练\n",
    "使用Hugging Face的Transformers库微调代码生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def fine_tune_code_model(train_data, model_name=\"microsoft/CodeGPT-small-py\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"code\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    tokenized_data = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, tokenizer\n",
    "\n",
    "# 使用示例（需要准备训练数据）\n",
    "fine_tuned_model, tokenizer = fine_tune_code_model(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 工程化实现\n",
    "### 7.1 API设计\n",
    "使用FastAPI构建API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class CodeQuery(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/generate_code/\")\n",
    "async def generate_code(query: CodeQuery):\n",
    "    # 1. 检索相关代码\n",
    "    similar_codes = retrieve_similar_codes(query.query, index, embeddings)\n",
    "\n",
    "    # 2. 使用微调后的模型生成代码\n",
    "    # （这里假设我们已经有了fine_tuned_model和tokenizer）\n",
    "    input_text = f\"Query: {query.query}\\nSimilar code: {similar_codes[0][1]}\\nGenerate:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = fine_tuned_model.generate(input_ids, max_length=200, num_return_sequences=1)\n",
    "    generated_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\"generated_code\": generated_code}\n",
    "\n",
    "# 运行服务器\n",
    "# uvicorn main:app --reload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
